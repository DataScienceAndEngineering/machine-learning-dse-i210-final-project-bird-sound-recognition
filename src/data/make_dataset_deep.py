# -*- coding: utf-8 -*-
"""make_dataset_deep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oVYjfR1EZC5e0K0piAdC5wXHacoL43uW
"""

import pandas as pd
import os
import numpy as np
import librosa
import tensorflow as tf
import matplotlib.pyplot as plt
import librosa.display
from sklearn.model_selection import train_test_split
from build_features_deepmodels import map_function_linear, map_function_mel, map_function_mfcc


def make_filepaths_labels(metadata_path, audio_path):
    metadata = pd.read_csv(metadata_path)

    file_paths = []

    audio_dir = audio_path

    for index, row in metadata.iterrows():
        file_path = os.path.join(audio_dir, row['filename'])
        if os.path.exists(file_path):
            file_paths.append(file_path)

    bird_labels = metadata['species']
    bird_labels = bird_labels.values
    bird_labels[bird_labels == ['bewickii']] = 0
    bird_labels[bird_labels == ['polyglottos']] = 1
    bird_labels[bird_labels == ['migratorius']] = 2
    bird_labels[bird_labels == ['melodia']] = 3
    bird_labels[bird_labels == ['cardinalis']] = 4
    bird_labels = np.squeeze(bird_labels)
    return file_paths, bird_labels

def make_dataset(feature, bird_labels, file_paths, shuffle):
    bird_labels = tf.convert_to_tensor(bird_labels, dtype = tf.int32)
    file_paths = tf.convert_to_tensor(file_paths, dtype = tf.string)

    bird_labels = tf.data.Dataset.from_tensor_slices(bird_labels)
    file_paths = tf.data.Dataset.from_tensor_slices(file_paths)
    dataset = tf.data.Dataset.zip( file_paths, bird_labels)

    if shuffle:
        dataset = dataset.shuffle(buffer_size = dataset.cardinality(), reshuffle_each_iteration=True)

    if feature == 'LinearSpectrogram':
        dataset = dataset.map(map_function_linear, num_parallel_calls = tf.data.AUTOTUNE)
    elif feature == 'MelSpectrogram':
        dataset = dataset.map(map_function_mel, num_parallel_calls = tf.data.AUTOTUNE)
    elif feature == 'MFCC':
        dataset = dataset.map(map_function_mfcc, num_parallel_calls = tf.data.AUTOTUNE)
    else:
        print("Not a valid feature, will extract MelSpectrogram by default")
        dataset = dataset.map(map_function_mel, num_parallel_calls = tf.data.AUTOTUNE)

    dataset = dataset.batch(batch_size = 64, num_parallel_calls = tf.data.AUTOTUNE, drop_remainder = True)

    return dataset

def train_test_split_deep(file_paths, y, feature_extract):
    feature = feature_extract
    file_paths_train, file_paths_test, y_train, y_test = train_test_split(
    file_paths, y, test_size=0.10, random_state=2419)

    dataset_train = make_dataset(feature, y_train, file_paths_train, shuffle=True)
    dataset_test = make_dataset(feature, y_test, file_paths_test, shuffle=False)
    return dataset_train, dataset_test

def main():
    features = ['LinearSpectrogram', 'MelSpectrogram', 'MFCC']
    metadata_path = '/content/drive/MyDrive/archive (17)/bird_songs_metadata.csv'
    audio_path = '/content/drive/MyDrive/archive (17)/wavfiles/'
    file_paths, labels = make_filepaths_labels(metadata_path, audio_path)
    dataset_train, dataset_test = train_test_split_deep(file_paths, labels, features[2])
    print(len(dataset_train))
    print(len(dataset_test))
    for batch in dataset_train.take(1):
      input_data_shape = batch[0].shape
      print("Shape of input data in training dataset:", input_data_shape)

if __name__ == "__main__":
    main()